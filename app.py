import streamlit as st
import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
import io

st.set_page_config(page_title="ØªØ­Ù„ÙŠÙ„ Ø³Ù„Ø© Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª - Ø¬Ø¯ÙŠØ¯", layout="wide", page_icon="ğŸ›’")
st.title("ğŸ›’ ØªØ·Ø¨ÙŠÙ‚ Ø¬Ø¯ÙŠØ¯ Ù„ØªØ­Ù„ÙŠÙ„ Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª")
st.markdown("Ø§Ø±ÙØ¹ Ù…Ù„Ù Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª (CSV)ØŒ Ø«Ù… Ø§Ø®ØªØ± Ø§Ù„Ø´Ù‡Ø± ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„ØªØ­Ù„ÙŠÙ„ Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª.")

@st.cache_data
def load_and_prepare_data(uploaded_file):
    if uploaded_file is None:
        return None, "ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„Ø§Ù‹."
    try:
        df = pd.read_csv(uploaded_file, encoding='utf-8')
    except Exception as e:
        return None, f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}"
    required_columns = ['Order Reference', 'product name', 'Order Date']
    if not all(col in df.columns for col in required_columns):
        return None, f"Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©: {', '.join(required_columns)}"
    df.dropna(axis=0, subset=['Order Reference', 'product name'], inplace=True)
    df['Order Reference'] = df['Order Reference'].astype(str)
    df['product name'] = df['product name'].str.strip().astype(str)
    df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')
    df.dropna(subset=['Order Date'], inplace=True)
    df['YearMonth'] = df['Order Date'].dt.to_period('M').astype(str)
    return df, None

def run_market_basket_analysis(df, target_month, min_support, min_confidence):
    monthly_df = df[df['YearMonth'] == target_month]
    if monthly_df.empty:
        return None, f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¨ÙŠØ¹Ø§Øª Ù„Ù„Ø´Ù‡Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯: {target_month}", "error"
    transactions = monthly_df.groupby('Order Reference')['product name'].apply(list).tolist()
    te = TransactionEncoder()
    te_ary = te.fit(transactions).transform(transactions)
    onehot_df = pd.DataFrame(te_ary, columns=te.columns_)
    frequent_itemsets = apriori(onehot_df, min_support=min_support, use_colnames=True)
    if frequent_itemsets.empty:
        return None, "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ù†Ù…Ø§Ø· Ø´Ø±Ø§Ø¡ Ù…ØªÙƒØ±Ø±Ø©. Ø­Ø§ÙˆÙ„ ØªÙ‚Ù„ÙŠÙ„ Ù‚ÙŠÙ…Ø© 'Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø¯Ø¹Ù…'.", "warning"
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=min_confidence)
    if rules.empty:
        return None, f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚ÙˆØ§Ø¹Ø¯ Ø±Ø¨Ø· Ù‚ÙˆÙŠØ© Ø¨Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© (Confidence > {min_confidence:.0%}). Ø­Ø§ÙˆÙ„ ØªÙ‚Ù„ÙŠÙ„ 'Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø«Ù‚Ø©'.", "info"
    rules['antecedents'] = rules['antecedents'].apply(lambda x: ', '.join(list(x)))
    rules['consequents'] = rules['consequents'].apply(lambda x: ', '.join(list(x)))
    results_df = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]
    results_df = results_df.sort_values(by='lift', ascending=False).reset_index(drop=True)
    results_df.rename(columns={
        'antecedents': 'Ø¥Ø°Ø§ Ø§Ø´ØªØ±Ù‰ Ø§Ù„Ø¹Ù…ÙŠÙ„ (Ø§Ù„Ø´Ø±Ø·)',
        'consequents': 'ÙØ¥Ù†Ù‡ ÙŠØ´ØªØ±ÙŠ Ø£ÙŠØ¶Ù‹Ø§ (Ø§Ù„Ù†ØªÙŠØ¬Ø©)',
        'support': 'Ù†Ø³Ø¨Ø© Ø§Ù„Ø¯Ø¹Ù…',
        'confidence': 'Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©',
        'lift': 'Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ù‚ÙˆØ© (Lift)'
    }, inplace=True)
    return results_df, None, "success"

def to_excel(df):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„')
        for column in df:
            column_width = max(df[column].astype(str).map(len).max(), len(column))
            writer.sheets['Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„'].set_column(df.columns.get_loc(column), df.columns.get_loc(column), column_width + 2)
    processed_data = output.getvalue()
    return processed_data

uploaded_file = st.file_uploader("1. Ø§Ø±ÙØ¹ Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª (CSV)", type=["csv"])
if uploaded_file is not None:
    df, error_msg = load_and_prepare_data(uploaded_file)
    if error_msg:
        st.error(error_msg)
    else:
        st.success("ØªÙ… ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
        available_months = sorted(df['YearMonth'].unique())
        target_month = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø´Ù‡Ø± Ù„Ù„ØªØ­Ù„ÙŠÙ„:", options=available_months)
        with st.expander("Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Ù…ØªÙ‚Ø¯Ù…)"):
            col1, col2 = st.columns(2)
            with col1:
                min_support_val = st.slider("Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø¯Ø¹Ù… (Support)", 0.001, 0.1, 0.01, 0.001, format="%.3f")
            with col2:
                min_confidence_val = st.slider("Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø«Ù‚Ø© (Confidence)", 0.1, 1.0, 0.5, 0.05, format="%.2f")
        if st.button("ğŸ” Ø§ÙƒØªØ´Ù Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª", type="primary"):
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª..."):
                result, msg, status = run_market_basket_analysis(df, target_month, min_support_val, min_confidence_val)
            st.header(f"3. Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø´Ù‡Ø±: {target_month}")
            if status == "success":
                st.success(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(result)} Ù‚Ø§Ø¹Ø¯Ø© Ù…Ù† Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù‚ÙˆÙŠØ©.")
                st.dataframe(result)
                excel_data = to_excel(result)
                st.download_button(
                    label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ…Ù„Ù Excel",
                    data=excel_data,
                    file_name=f"ØªØ­Ù„ÙŠÙ„_Ø³Ù„Ø©_Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª_{target_month}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            elif status == "error":
                st.error(msg)
            elif status == "warning":
                st.warning(msg)
            else:
                st.info(msg)
else:
    st.info("ÙÙŠ Ø§Ù†ØªØ¸Ø§Ø± Ø±ÙØ¹ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
